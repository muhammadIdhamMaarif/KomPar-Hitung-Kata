



Abstract
We introduce the Segment Anything (SA) project: a new
task, model, and dataset for image segmentation. Using our
efficient model in a data collection loop, we built the largest
segmentation dataset to date (by far), with over 1 billion
masks on 11M licensed and privacy respecting images. The
model is designed and trained to be promptable, so it can
transfer zero-shot to new image distributions and tasks. We
evaluate its capabilities on numerous tasks and find that
its zero-shot performance is impressive – often competitive
with or even superior to prior fully supervised results. We
are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at
https://segment-anything.com to foster research into foundation models for computer vision.
1. Introduction
Large language models pre-trained on web-scale datasets
are revolutionizing NLP with strong zero-shot and few-shot
generalization [10]. These “foundation models” [8] can
generalize to tasks and data distributions beyond those seen
during training. This capability is often implemented with
prompt engineering in which hand-crafted text is used to
prompt the language model to generate a valid textual response for the task at hand. When scaled and trained with
abundant text corpora from the web, these models’ zero and
few-shot performance compares surprisingly well to (even
matching in some cases) fine-tuned models [10, 21]. Empirical trends show this behavior improving with model scale,
dataset size, and total training compute [56, 10, 21, 51].
Foundation models have also been explored in computer
vision, albeit to a lesser extent. Perhaps the most prominent illustration aligns paired text and images from the web.
For example, CLIP [82] and ALIGN [55] use contrastive
learning to train text and image encoders that align the two
modalities. Once trained, engineered text prompts enable
zero-shot generalization to novel visual concepts and data
distributions. Such encoders also compose effectively with
other modules to enable downstream tasks, such as image
generation (e.g., DALL·E [83]). While much progress has
been made on vision and language encoders, computer vision includes a wide range of problems beyond this scope,
and for many of these, abundant training data does not exist.
In this work, our goal is to build a foundation model for
image segmentation. That is, we seek to develop a promptable model and pre-train it on a broad dataset using a task
that enables powerful generalization. With this model, we
aim to solve a range of downstream segmentation problems
on new data distributions using prompt engineering.
The success of this plan hinges on three components:
task, model, and data. To develop them, we address the
following questions about image segmentation:
1. What task will enable zero-shot generalization?
2. What is the corresponding model architecture?
3. What data can power this task and model?
